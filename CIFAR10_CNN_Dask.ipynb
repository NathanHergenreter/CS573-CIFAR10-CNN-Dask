{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352a566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "from CIFAR10_CNN import cifar10_cnn\n",
    "from CIFAR10_CNN_Improved import cifar10_cnn_improved\n",
    "from CIFAR10_CNN_Distributed import cifar10_cnn_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daabc557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distributed model training...\n",
      "Starting distributed model training (epochs=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  _score\n",
      "args:      ((KerasClassifier(\n",
      "\tmodel=<function define_model_improved at 0x0000029038B03DC8>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=False\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tlearning_rate=0.07269712277405341\n",
      "\tmomentum=0.11706618702172067\n",
      "\tclass_weight=None\n",
      "), {'model_id': 3, 'params': {'batch_size': 64, 'epochs': 1, 'learning_rate': 0.07269712277405341, 'momentum': 0.11706618702172067}, 'partial_fit_calls': 1, 'partial_fit_time': 741.5639660358429}), array([[[[0.5176471 , 0.41568628, 0.18431373],\n",
      "         [0.627451  , 0.5254902 , 0.25882354],\n",
      "         [0.6627451 , 0.63529414, 0.22352941],\n",
      "         ...,\n",
      "         [0.7176471 , 0.627451  , 0.3137255 ],\n",
      "         [0.6862745 , 0.6       , 0.27450982],\n",
      "         [0.69411767, 0.60784316, 0.28235295]],\n",
      "\n",
      "        [[0.5686275 , 0.4627451 , 0.2       ],\n",
      "         [0.67058825, 0.5647059 , 0.27450982],\n",
      "         [\n",
      "kwargs:    {}\n",
      "Exception: 'TypeError(\"\\'NoneType\\' object is not callable\")'\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4524\\2383062565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting distributed model training (epochs=1)...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel_1_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No dask completion time (epochs=1): '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Coursework\\CS573\\Project\\CIFAR10_CNN.py\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m(self, load_model, epochs, batch_size, learning_rate, momentum)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Coursework\\CS573\\Project\\CIFAR10_CNN_Distributed.py\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(self, trainX, trainY, epochs, batch_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloguniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHyperbandSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_client\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36msync\u001b[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             return sync(\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m             )\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\distributed\\utils.py\u001b[0m in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\distributed\\utils.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_hyperband.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         _SHAs = await asyncio.gather(\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSHAs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_brackets_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         )\n\u001b[0;32m    404\u001b[0m         \u001b[0mSHAs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSHA\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSHA\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_brackets_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SHAs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    673\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m                 \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m             )\n\u001b[0;32m    677\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, params, X_train, y_train, X_test, y_test, additional_calls, fit_params, scorer, random_state, verbose, prefix)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m         \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(model, params, X_train, y_train, X_test, y_test, additional_calls, fit_params, scorer, random_state, verbose, prefix)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;31m# async for future, result in seq:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mmetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlog_delay\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_i\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_delay\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36m_gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   1832\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m                             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1835\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"skip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_and_meta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         )\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m()\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return self._sign * self._score_func(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \"\"\"\n\u001b[0;32m   1009\u001b[0m         \u001b[1;31m# predict with Keras model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;31m# post process y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_predict_raw\u001b[1;34m()\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;31m# predict with Keras model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpred_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1980\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  _score\n",
      "args:      ((KerasClassifier(\n",
      "\tmodel=<function define_model_improved at 0x0000029038B03DC8>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=False\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tlearning_rate=0.020195367850306834\n",
      "\tmomentum=0.8388640303632403\n",
      "\tclass_weight=None\n",
      "), {'model_id': 2, 'params': {'batch_size': 64, 'epochs': 1, 'learning_rate': 0.020195367850306834, 'momentum': 0.8388640303632403}, 'partial_fit_calls': 1, 'partial_fit_time': 745.8535649776459}), array([[[[0.49019608, 0.65882355, 0.8666667 ],\n",
      "         [0.49411765, 0.65882355, 0.8666667 ],\n",
      "         [0.49803922, 0.6666667 , 0.87058824],\n",
      "         ...,\n",
      "         [0.50980395, 0.67058825, 0.8784314 ],\n",
      "         [0.5058824 , 0.67058825, 0.8784314 ],\n",
      "         [0.49803922, 0.68235296, 0.8784314 ]],\n",
      "\n",
      "        [[0.49803922, 0.67058825, 0.8784314 ],\n",
      "         [0.5019608 , 0.67058825, 0.8784314 ],\n",
      "         [\n",
      "kwargs:    {}\n",
      "Exception: 'TypeError(\"\\'NoneType\\' object is not callable\")'\n",
      "\n",
      "distributed.utils - ERROR - \"Failed to add concrete function 'b'__inference_predict_function_23752'' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\distributed\\utils.py\", line 648, in log_errors\n",
      "    yield\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\", line 102, in _partial_fit\n",
      "    model = deepcopy(model)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 180, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 281, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 150, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 241, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 161, in deepcopy\n",
      "    y = copier(memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\scikeras\\_saving_utils.py\", line 117, in deepcopy_model\n",
      "    _, (model_bytes, optimizer_weights) = pack_keras_model(model)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\scikeras\\_saving_utils.py\", line 89, in pack_keras_model\n",
      "    model.save(temp_dir)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_serialization.py\", line 64, in serialize_concrete_function\n",
      "    f\"Failed to add concrete function '{concrete_function.name}' to object-\"\n",
      "KeyError: \"Failed to add concrete function 'b'__inference_predict_function_23752'' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  execute_task\n",
      "args:      ((<function _partial_fit at 0x0000029038AAF1F8>, (KerasClassifier(\n",
      "\tmodel=<function define_model_improved at 0x0000029038B03DC8>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=False\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tlearning_rate=0.07269712277405341\n",
      "\tmomentum=0.11706618702172067\n",
      "\tclass_weight=None\n",
      "), {'model_id': 3, 'params': {'batch_size': 64, 'epochs': 1, 'learning_rate': 0.07269712277405341, 'momentum': 0.11706618702172067}, 'partial_fit_calls': 1, 'partial_fit_time': 741.5639660358429}), array([[[[0.01176471, 0.01176471, 0.01176471],\n",
      "         [0.01176471, 0.01176471, 0.01176471],\n",
      "         [0.01176471, 0.01176471, 0.01176471],\n",
      "         ...,\n",
      "         [0.10588235, 0.1764706 , 0.22352941],\n",
      "         [0.16078432, 0.24705882, 0.30588236],\n",
      "         [0.18039216, 0.28627452, 0.36078432]],\n",
      "\n",
      "        [[0.01176471, 0.01176471, 0.01176471],\n",
      "         \n",
      "kwargs:    {}\n",
      "Exception: 'KeyError(\"Failed to add concrete function \\'b\\'__inference_predict_function_23752\\'\\' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\")'\n",
      "\n",
      "distributed.utils - ERROR - \"Failed to add concrete function 'b'__inference_predict_function_27755'' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\distributed\\utils.py\", line 648, in log_errors\n",
      "    yield\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_incremental.py\", line 102, in _partial_fit\n",
      "    model = deepcopy(model)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 180, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 281, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 150, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 241, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\copy.py\", line 161, in deepcopy\n",
      "    y = copier(memo)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\scikeras\\_saving_utils.py\", line 117, in deepcopy_model\n",
      "    _, (model_bytes, optimizer_weights) = pack_keras_model(model)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\scikeras\\_saving_utils.py\", line 89, in pack_keras_model\n",
      "    model.save(temp_dir)\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_serialization.py\", line 64, in serialize_concrete_function\n",
      "    f\"Failed to add concrete function '{concrete_function.name}' to object-\"\n",
      "KeyError: \"Failed to add concrete function 'b'__inference_predict_function_27755'' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\n",
      "distributed.worker - WARNING - Compute Failed\n",
      "Function:  execute_task\n",
      "args:      ((<function _partial_fit at 0x0000029038AAF1F8>, (KerasClassifier(\n",
      "\tmodel=<function define_model_improved at 0x0000029038B03DC8>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=64\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=False\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tlearning_rate=0.020195367850306834\n",
      "\tmomentum=0.8388640303632403\n",
      "\tclass_weight=None\n",
      "), {'model_id': 2, 'params': {'batch_size': 64, 'epochs': 1, 'learning_rate': 0.020195367850306834, 'momentum': 0.8388640303632403}, 'partial_fit_calls': 1, 'partial_fit_time': 745.8535649776459}), array([[[[0.34901962, 0.49019608, 0.5411765 ],\n",
      "         [0.23137255, 0.3529412 , 0.4117647 ],\n",
      "         [0.21568628, 0.32156864, 0.38039216],\n",
      "         ...,\n",
      "         [0.34509805, 0.5019608 , 0.56078434],\n",
      "         [0.33333334, 0.4862745 , 0.54509807],\n",
      "         [0.30980393, 0.45882353, 0.5058824 ]],\n",
      "\n",
      "        [[0.27058825, 0.4       , 0.44705883],\n",
      "         \n",
      "kwargs:    {}\n",
      "Exception: 'KeyError(\"Failed to add concrete function \\'b\\'__inference_predict_function_27755\\'\\' to object-based SavedModel as it captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\")'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\megal\\AppData\\Local\\Temp\\tmp1j8v6gwd\\assets\n"
     ]
    }
   ],
   "source": [
    "client = Client(processes=False)             # create local cluster\n",
    "# client = Client(\"scheduler-address:8786\")  # or connect to remote cluster\n",
    "\n",
    "# Dask parallel\n",
    "print('Starting distributed model training...')\n",
    "start = time.time()\n",
    "with joblib.parallel_backend('dask'):\n",
    "    model_1_dist = cifar10_cnn_distributed('cnn_model_1epochs_dist.h5')\n",
    "    model_10_dist = cifar10_cnn_distributed('cnn_model_10epochs_dist.h5')\n",
    "    model_50_dist = cifar10_cnn_distributed('cnn_model_50epochs_dist.h5')\n",
    "    #model_100_dist = cifar10_cnn_distributed('cnn_model_100epochs_dist.h5')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=1)...')\n",
    "    start = time.time()\n",
    "    model_1_dist.run_test_harness(epochs=1, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=1): ', end - start, 'seconds')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=10)...')\n",
    "    start = time.time()\n",
    "    model_10_dist.run_test_harness(epochs=10, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=10): ', end - start, 'seconds')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=50)...')\n",
    "    start = time.time()\n",
    "    #model_50_dist.run_test_harness(epochs=50, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=50): ', end - start, 'seconds')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=100)...')\n",
    "    start = time.time()\n",
    "    #model_100_dist.run_test_harness(epochs=100, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=100): ', end - start, 'seconds')\n",
    "    \n",
    "print('Distributed models done')\n",
    "\n",
    "#No dask completion time (epochs=10):  2111.384060382843 seconds\n",
    "#Starting model training (epochs=50)...\n",
    "#No dask completion time (epochs=50):  8861.92734336853 seconds\n",
    "#Starting model training (epochs=100)...\n",
    "#Starting distributed model training (epochs=10)...\n",
    "#No dask completion time (epochs=10):  1963.2309262752533 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1142ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting non-distributed event training...\n",
      "Starting event model training (epochs=1)...\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 58500 files for training.\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 6500 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megal\\Coursework\\CS573\\Project\\CIFAR10_CNN.py:72: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  trainX = np.array(list(map(lambda x: x[0], train_numpy)))\n",
      "C:\\Users\\megal\\Coursework\\CS573\\Project\\CIFAR10_CNN.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  trainY = np.array(list(map(lambda x: x[1], train_numpy)))\n",
      "C:\\Users\\megal\\Coursework\\CS573\\Project\\CIFAR10_CNN.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  testX = np.array(list(map(lambda x: x[0], test_numpy)))\n",
      "C:\\Users\\megal\\Coursework\\CS573\\Project\\CIFAR10_CNN.py:79: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  testY = np.array(list(map(lambda x: x[1], test_numpy)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 13.646\n",
      "Starting event model training (epochs=10)...\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 58500 files for training.\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 6500 files for validation.\n",
      "> 13.769\n",
      "Non-distributed event model completion time (epochs=10):  2691.480133533478 seconds\n",
      "Starting event model training (epochs=50)...\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 58500 files for training.\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 6500 files for validation.\n",
      "> 14.631\n",
      "Non-distributed event model completion time (epochs=50):  13552.021950244904 seconds\n",
      "Starting event model training (epochs=100)...\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 58500 files for training.\n",
      "Found 65000 files belonging to 10 classes.\n",
      "Using 6500 files for validation.\n",
      "> 14.631\n",
      "Non-distributed event model completion time (epochs=100):  66.64784717559814 seconds\n"
     ]
    }
   ],
   "source": [
    "model_event_1 = cifar10_cnn_improved('cnn_event_model_1epochs.h5', 'data_img')\n",
    "model_event_10 = cifar10_cnn_improved('cnn_event_model_10epochs.h5', 'data_img')\n",
    "model_event_50 = cifar10_cnn_improved('cnn_event_model_50epochs.h5', 'data_img')\n",
    "model_event_100 = cifar10_cnn_improved('cnn_event_model_100epochs.h5', 'data_img')\n",
    "#model_dask = cifar10_cnn_dask()\n",
    "\n",
    "# No dask no parallel event images\n",
    "print('Starting non-distributed event training...')\n",
    "# epochs = 1\n",
    "print('Starting event model training (epochs=1)...')\n",
    "start = time.time()\n",
    "model_event_1.run_test_harness(epochs=1, batch_size=64)\n",
    "end = time.time()\n",
    "# epochs = 10\n",
    "print('Starting event model training (epochs=10)...')\n",
    "start = time.time()\n",
    "model_event_10.run_test_harness(epochs=10, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=10): ', end - start, 'seconds')\n",
    "# epochs = 50\n",
    "print('Starting event model training (epochs=50)...')\n",
    "start = time.time()\n",
    "model_event_50.run_test_harness(epochs=50, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=50): ', end - start, 'seconds')\n",
    "# epochs = 100\n",
    "print('Starting event model training (epochs=100)...')\n",
    "start = time.time()\n",
    "model_event_50.run_test_harness(epochs=100, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=100): ', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832a1a0",
   "metadata": {},
   "source": [
    "Starting non-distributed event training...\n",
    "Starting event model training (epochs=1)...\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 58500 files for training.\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 6500 files for validation.\n",
    "> 13.646\n",
    "Starting event model training (epochs=10)...\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 58500 files for training.\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 6500 files for validation.\n",
    "> 13.769\n",
    "Non-distributed event model completion time (epochs=10):  2691.480133533478 seconds\n",
    "Starting event model training (epochs=50)...\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 58500 files for training.\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 6500 files for validation.\n",
    "> 14.631\n",
    "Non-distributed event model completion time (epochs=50):  13552.021950244904 seconds\n",
    "Starting event model training (epochs=100)...\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 58500 files for training.\n",
    "Found 65000 files belonging to 10 classes.\n",
    "Using 6500 files for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313e7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "(9000, 10)\n",
      "(10,)\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import numpy as np\n",
    "\n",
    "train = image_dataset_from_directory(\n",
    "    'data_img',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    seed=0,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(32, 32)\n",
    ")\n",
    "test = image_dataset_from_directory(\n",
    "    'data_img',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    seed=0,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(32, 32)\n",
    ")\n",
    "\n",
    "#normalization_layer = Rescaling(1./255)\n",
    "#normalized_ds = train.map(lambda x, y: (normalization_layer(x), y))\n",
    "#image_batch, labels_batch = next(iter(normalized_ds))\n",
    "#first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "#print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "#train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#test = test.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_numpy = tfds.as_numpy(train)  # Convert `tf.data.Dataset` to Python generator\n",
    "\n",
    "X_train = np.array(list(map(lambda x: x[0], train_numpy)))\n",
    "y_train = np.array(list(map(lambda x: x[1], train_numpy)))\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train[0].shape)\n",
    "\n",
    "y_train = np.concatenate(y_train[:])\n",
    "#y_train = y_train.reshape((len(y_train), 1))\n",
    "print(y_train.shape)\n",
    "print(y_train[0].shape)\n",
    "print(y_train)\n",
    "\n",
    "#X_train = np.concatenate(X_train[:])\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train[0].shape)\n",
    "#train_norm = X_train.astype('float32')\n",
    "#for ex in ds_numpy:\n",
    "    # `{'image': np.array(shape=(28, 28, 1)), 'labels': np.array(shape=())}`\n",
    "#    print(ex[1])\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e0b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "(50000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "print(trainY.shape)\n",
    "trainY = to_categorical(trainY)\n",
    "print(trainY.shape)\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0330714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
