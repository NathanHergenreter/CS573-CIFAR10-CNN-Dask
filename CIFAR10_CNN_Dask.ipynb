{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352a566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_validate_shuffle_split_init' from 'sklearn.model_selection._split' (C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39852\\1511126786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mCIFAR10_CNN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10_cnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mCIFAR10_CNN_Distributed\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10_cnn_distributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Coursework\\CS573\\Project\\CIFAR10_CNN_Distributed.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHyperbandSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloguniform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mCIFAR10_CNN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10_cnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_n_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask_ml\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m from sklearn.model_selection._split import (\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mBaseCrossValidator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0m_validate_shuffle_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_validate_shuffle_split_init' from 'sklearn.model_selection._split' (C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "from CIFAR10_CNN import cifar10_cnn\n",
    "from CIFAR10_CNN_Distributed import cifar10_cnn_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daabc557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megal\\Anaconda3\\lib\\site-packages\\distributed\\scheduler.py:1245: UserWarning: \n",
      "Could not launch service 'dashboard' on port 8787. Got the following message:\n",
      "\n",
      "failed to validate _ServerOpts(...).prefix: expected a value of type str, got None of type NoneType\n",
      "  self.start()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting non-distributed training...\n",
      "Starting model training (epochs=10)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10672\\3759782564.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting model training (epochs=10)...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Non-distributed completion time (epochs=10): '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Coursework\\CS573\\Project\\CIFAR10_CNN.py\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m(self, load_model, epochs, batch_size, lr, momentum)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Coursework\\CS573\\Project\\CIFAR10_CNN.py\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(self, trainX, trainY, epochs, batch_size)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client = Client(processes=False)             # create local cluster\n",
    "# client = Client(\"scheduler-address:8786\")  # or connect to remote cluster\n",
    "\n",
    "model_10 = cifar10_cnn('cnn_model_10epochs.h5')\n",
    "model_50 = cifar10_cnn('cnn_model_50epochs.h5')\n",
    "model_100 = cifar10_cnn('cnn_model_100epochs.h5')\n",
    "#model_dask = cifar10_cnn_dask()\n",
    "\n",
    "# No dask no parallel\n",
    "# epochs = 10\n",
    "print('Starting non-distributed training...')\n",
    "print('Starting model training (epochs=10)...')\n",
    "start = time.time()\n",
    "#model_10.run_test_harness(epochs=10, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed completion time (epochs=10): ', end - start, 'seconds')\n",
    "\n",
    "# epochs = 50\n",
    "print('Starting model training (epochs=50)...')\n",
    "start = time.time()\n",
    "#model_50.run_test_harness(epochs=50, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed completion time (epochs=50): ', end - start, 'seconds')\n",
    "\n",
    "# epochs = 100\n",
    "print('Starting model training (epochs=100)...')\n",
    "start = time.time()\n",
    "#model_100.run_test_harness(epochs=100, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed completion time (epochs=100): ', end - start, 'seconds')\n",
    "print()\n",
    "\n",
    "# Dask parallel\n",
    "print('Starting distributed model training...')\n",
    "start = time.time()\n",
    "with joblib.parallel_backend('dask'):\n",
    "    model_10_dist = cifar10_cnn_distributed('cnn_model_10epochs_dist.h5')\n",
    "    model_50_dist = cifar10_cnn_distributed('cnn_model_50epochs_dist.h5')\n",
    "    model_100_dist = cifar10_cnn_distributed('cnn_model_100epochs_dist.h5')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=10)...')\n",
    "    start = time.time()\n",
    "    model_10_dist.run_test_harness(epochs=10, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=10): ', end - start, 'seconds')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=50)...')\n",
    "    start = time.time()\n",
    "    #model_50_dist.run_test_harness(epochs=50, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=50): ', end - start, 'seconds')\n",
    "    \n",
    "    print('Starting distributed model training (epochs=100)...')\n",
    "    start = time.time()\n",
    "    #model_100_dist.run_test_harness(epochs=100, batch_size=64)\n",
    "    end = time.time()\n",
    "    print('No dask completion time (epochs=100): ', end - start, 'seconds')\n",
    "    \n",
    "print('Distributed models done')\n",
    "\n",
    "#No dask completion time (epochs=10):  2111.384060382843 seconds\n",
    "#Starting model training (epochs=50)...\n",
    "#No dask completion time (epochs=50):  8861.92734336853 seconds\n",
    "#Starting model training (epochs=100)...\n",
    "#Starting distributed model training (epochs=10)...\n",
    "#No dask completion time (epochs=10):  1963.2309262752533 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1142ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting non-distributed event training...\n",
      "Starting event model training (epochs=10)...\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "> 10.500\n",
      "Non-distributed event model completion time (epochs=10):  28.79258418083191 seconds\n",
      "Starting event model training (epochs=50)...\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "> 10.000\n",
      "Non-distributed event model completion time (epochs=50):  22468.89160966873 seconds\n",
      "Starting event model training (epochs=100)...\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "> 10.000\n",
      "Non-distributed event model completion time (epochs=100):  22.712862968444824 seconds\n"
     ]
    }
   ],
   "source": [
    "model_event_10 = cifar10_cnn('cnn_event_model_10epochs.h5', 'data_img')\n",
    "model_event_50 = cifar10_cnn('cnn_event_model_50epochs.h5', 'data_img')\n",
    "model_event_100 = cifar10_cnn('cnn_event_model_100epochs.h5', 'data_img')\n",
    "#model_dask = cifar10_cnn_dask()\n",
    "\n",
    "# No dask no parallel event images\n",
    "print('Starting non-distributed event training...')\n",
    "# epochs = 10\n",
    "print('Starting event model training (epochs=10)...')\n",
    "start = time.time()\n",
    "model_event_10.run_test_harness(epochs=10, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=10): ', end - start, 'seconds')\n",
    "# epochs = 50\n",
    "print('Starting event model training (epochs=50)...')\n",
    "start = time.time()\n",
    "model_event_50.run_test_harness(epochs=50, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=50): ', end - start, 'seconds')\n",
    "# epochs = 100\n",
    "print('Starting event model training (epochs=100)...')\n",
    "start = time.time()\n",
    "model_event_50.run_test_harness(epochs=100, batch_size=64)\n",
    "end = time.time()\n",
    "print('Non-distributed event model completion time (epochs=100): ', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313e7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "(9000, 10)\n",
      "(10,)\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import numpy as np\n",
    "\n",
    "train = image_dataset_from_directory(\n",
    "    'data_img',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    "    seed=0,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(32, 32)\n",
    ")\n",
    "test = image_dataset_from_directory(\n",
    "    'data_img',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    seed=0,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    image_size=(32, 32)\n",
    ")\n",
    "\n",
    "#normalization_layer = Rescaling(1./255)\n",
    "#normalized_ds = train.map(lambda x, y: (normalization_layer(x), y))\n",
    "#image_batch, labels_batch = next(iter(normalized_ds))\n",
    "#first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "#print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "#train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#test = test.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_numpy = tfds.as_numpy(train)  # Convert `tf.data.Dataset` to Python generator\n",
    "\n",
    "X_train = np.array(list(map(lambda x: x[0], train_numpy)))\n",
    "y_train = np.array(list(map(lambda x: x[1], train_numpy)))\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train[0].shape)\n",
    "\n",
    "y_train = np.concatenate(y_train[:])\n",
    "#y_train = y_train.reshape((len(y_train), 1))\n",
    "print(y_train.shape)\n",
    "print(y_train[0].shape)\n",
    "print(y_train)\n",
    "\n",
    "#X_train = np.concatenate(X_train[:])\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_train[0].shape)\n",
    "#train_norm = X_train.astype('float32')\n",
    "#for ex in ds_numpy:\n",
    "    # `{'image': np.array(shape=(28, 28, 1)), 'labels': np.array(shape=())}`\n",
    "#    print(ex[1])\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e0b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "(50000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "print(trainY.shape)\n",
    "trainY = to_categorical(trainY)\n",
    "print(trainY.shape)\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e803917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "> 10.500\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 9000 files for training.\n",
      "Found 10000 files belonging to 10 classes.\n",
      "Using 1000 files for validation.\n",
      "[[[[ 45.]\n",
      "   [144.]\n",
      "   [ 58.]\n",
      "   ...\n",
      "   [ 49.]\n",
      "   [205.]\n",
      "   [ 75.]]\n",
      "\n",
      "  [[ 48.]\n",
      "   [ 59.]\n",
      "   [112.]\n",
      "   ...\n",
      "   [ 50.]\n",
      "   [ 17.]\n",
      "   [ 43.]]\n",
      "\n",
      "  [[ 93.]\n",
      "   [139.]\n",
      "   [  3.]\n",
      "   ...\n",
      "   [139.]\n",
      "   [ 36.]\n",
      "   [ 16.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.]\n",
      "   [  0.]\n",
      "   [253.]\n",
      "   ...\n",
      "   [ 79.]\n",
      "   [241.]\n",
      "   [246.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [  0.]\n",
      "   [ 39.]\n",
      "   ...\n",
      "   [242.]\n",
      "   [241.]\n",
      "   [239.]]\n",
      "\n",
      "  [[ 36.]\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [255.]\n",
      "   [242.]\n",
      "   [228.]]]\n",
      "\n",
      "\n",
      " [[[  0.]\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [182.]\n",
      "   [ 14.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [248.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [ 14.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[  1.]\n",
      "   [  1.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  2.]\n",
      "   [  0.]\n",
      "   [ 14.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[208.]\n",
      "   [235.]\n",
      "   [254.]\n",
      "   ...\n",
      "   [250.]\n",
      "   [239.]\n",
      "   [233.]]\n",
      "\n",
      "  [[217.]\n",
      "   [253.]\n",
      "   [227.]\n",
      "   ...\n",
      "   [243.]\n",
      "   [243.]\n",
      "   [244.]]\n",
      "\n",
      "  [[253.]\n",
      "   [250.]\n",
      "   [205.]\n",
      "   ...\n",
      "   [249.]\n",
      "   [239.]\n",
      "   [250.]]]\n",
      "\n",
      "\n",
      " [[[ 84.]\n",
      "   [ 21.]\n",
      "   [ 96.]\n",
      "   ...\n",
      "   [ 49.]\n",
      "   [ 96.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[ 62.]\n",
      "   [  0.]\n",
      "   [242.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[ 52.]\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [154.]\n",
      "   [  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7.]\n",
      "   [  0.]\n",
      "   [  1.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  1.]\n",
      "   [  1.]]\n",
      "\n",
      "  [[ 10.]\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [ 22.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[  1.]\n",
      "   [  0.]\n",
      "   [  2.]\n",
      "   ...\n",
      "   [ 38.]\n",
      "   [ 17.]\n",
      "   [  0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 10.]\n",
      "   [ 79.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [152.]\n",
      "   [  3.]\n",
      "   [ 60.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [ 69.]\n",
      "   [ 60.]\n",
      "   ...\n",
      "   [  1.]\n",
      "   [ 27.]\n",
      "   [ 65.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [  0.]\n",
      "   [200.]\n",
      "   ...\n",
      "   [ 21.]\n",
      "   [  9.]\n",
      "   [118.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[250.]\n",
      "   [171.]\n",
      "   [156.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [ 35.]\n",
      "   [226.]]\n",
      "\n",
      "  [[ 80.]\n",
      "   [ 25.]\n",
      "   [205.]\n",
      "   ...\n",
      "   [ 61.]\n",
      "   [  0.]\n",
      "   [ 24.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [191.]\n",
      "   [ 16.]\n",
      "   ...\n",
      "   [255.]\n",
      "   [204.]\n",
      "   [ 22.]]]\n",
      "\n",
      "\n",
      " [[[ 45.]\n",
      "   [251.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [ 88.]]\n",
      "\n",
      "  [[156.]\n",
      "   [ 85.]\n",
      "   [127.]\n",
      "   ...\n",
      "   [111.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[ 45.]\n",
      "   [204.]\n",
      "   [ 21.]\n",
      "   ...\n",
      "   [196.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[232.]\n",
      "   [123.]\n",
      "   [221.]\n",
      "   ...\n",
      "   [  1.]\n",
      "   [159.]\n",
      "   [ 22.]]\n",
      "\n",
      "  [[  1.]\n",
      "   [ 88.]\n",
      "   [163.]\n",
      "   ...\n",
      "   [228.]\n",
      "   [ 89.]\n",
      "   [206.]]\n",
      "\n",
      "  [[ 77.]\n",
      "   [102.]\n",
      "   [206.]\n",
      "   ...\n",
      "   [182.]\n",
      "   [  2.]\n",
      "   [  7.]]]\n",
      "\n",
      "\n",
      " [[[ 30.]\n",
      "   [  0.]\n",
      "   [  1.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[227.]\n",
      "   [185.]\n",
      "   [ 22.]\n",
      "   ...\n",
      "   [  1.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [ 10.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [  1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[252.]\n",
      "   [168.]\n",
      "   [242.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [  0.]\n",
      "   [  0.]]\n",
      "\n",
      "  [[244.]\n",
      "   [242.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [  0.]\n",
      "   [126.]\n",
      "   [159.]]\n",
      "\n",
      "  [[  0.]\n",
      "   [199.]\n",
      "   [  0.]\n",
      "   ...\n",
      "   [ 73.]\n",
      "   [  0.]\n",
      "   [  0.]]]]\n",
      "[[[[0.1764706 ]\n",
      "   [0.5647059 ]\n",
      "   [0.22745098]\n",
      "   ...\n",
      "   [0.19215687]\n",
      "   [0.8039216 ]\n",
      "   [0.29411766]]\n",
      "\n",
      "  [[0.1882353 ]\n",
      "   [0.23137255]\n",
      "   [0.4392157 ]\n",
      "   ...\n",
      "   [0.19607843]\n",
      "   [0.06666667]\n",
      "   [0.16862746]]\n",
      "\n",
      "  [[0.3647059 ]\n",
      "   [0.54509807]\n",
      "   [0.01176471]\n",
      "   ...\n",
      "   [0.54509807]\n",
      "   [0.14117648]\n",
      "   [0.0627451 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.99215686]\n",
      "   ...\n",
      "   [0.30980393]\n",
      "   [0.94509804]\n",
      "   [0.9647059 ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.15294118]\n",
      "   ...\n",
      "   [0.9490196 ]\n",
      "   [0.94509804]\n",
      "   [0.9372549 ]]\n",
      "\n",
      "  [[0.14117648]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [0.9490196 ]\n",
      "   [0.89411765]]]\n",
      "\n",
      "\n",
      " [[[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.7137255 ]\n",
      "   [0.05490196]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.972549  ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.05490196]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.00392157]\n",
      "   [0.00392157]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.00784314]\n",
      "   [0.        ]\n",
      "   [0.05490196]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8156863 ]\n",
      "   [0.92156863]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.98039216]\n",
      "   [0.9372549 ]\n",
      "   [0.9137255 ]]\n",
      "\n",
      "  [[0.8509804 ]\n",
      "   [0.99215686]\n",
      "   [0.8901961 ]\n",
      "   ...\n",
      "   [0.9529412 ]\n",
      "   [0.9529412 ]\n",
      "   [0.95686275]]\n",
      "\n",
      "  [[0.99215686]\n",
      "   [0.98039216]\n",
      "   [0.8039216 ]\n",
      "   ...\n",
      "   [0.9764706 ]\n",
      "   [0.9372549 ]\n",
      "   [0.98039216]]]\n",
      "\n",
      "\n",
      " [[[0.32941177]\n",
      "   [0.08235294]\n",
      "   [0.3764706 ]\n",
      "   ...\n",
      "   [0.19215687]\n",
      "   [0.3764706 ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.24313726]\n",
      "   [0.        ]\n",
      "   [0.9490196 ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.20392157]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.6039216 ]\n",
      "   [0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02745098]\n",
      "   [0.        ]\n",
      "   [0.00392157]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.00392157]\n",
      "   [0.00392157]]\n",
      "\n",
      "  [[0.03921569]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.08627451]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.00392157]\n",
      "   [0.        ]\n",
      "   [0.00784314]\n",
      "   ...\n",
      "   [0.14901961]\n",
      "   [0.06666667]\n",
      "   [0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.03921569]\n",
      "   [0.30980393]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.59607846]\n",
      "   [0.01176471]\n",
      "   [0.23529412]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.27058825]\n",
      "   [0.23529412]\n",
      "   ...\n",
      "   [0.00392157]\n",
      "   [0.10588235]\n",
      "   [0.25490198]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.78431374]\n",
      "   ...\n",
      "   [0.08235294]\n",
      "   [0.03529412]\n",
      "   [0.4627451 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.98039216]\n",
      "   [0.67058825]\n",
      "   [0.6117647 ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.13725491]\n",
      "   [0.8862745 ]]\n",
      "\n",
      "  [[0.3137255 ]\n",
      "   [0.09803922]\n",
      "   [0.8039216 ]\n",
      "   ...\n",
      "   [0.23921569]\n",
      "   [0.        ]\n",
      "   [0.09411765]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.7490196 ]\n",
      "   [0.0627451 ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [0.8       ]\n",
      "   [0.08627451]]]\n",
      "\n",
      "\n",
      " [[[0.1764706 ]\n",
      "   [0.9843137 ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.34509805]]\n",
      "\n",
      "  [[0.6117647 ]\n",
      "   [0.33333334]\n",
      "   [0.49803922]\n",
      "   ...\n",
      "   [0.43529412]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.1764706 ]\n",
      "   [0.8       ]\n",
      "   [0.08235294]\n",
      "   ...\n",
      "   [0.76862746]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9098039 ]\n",
      "   [0.48235294]\n",
      "   [0.8666667 ]\n",
      "   ...\n",
      "   [0.00392157]\n",
      "   [0.62352943]\n",
      "   [0.08627451]]\n",
      "\n",
      "  [[0.00392157]\n",
      "   [0.34509805]\n",
      "   [0.6392157 ]\n",
      "   ...\n",
      "   [0.89411765]\n",
      "   [0.34901962]\n",
      "   [0.80784315]]\n",
      "\n",
      "  [[0.3019608 ]\n",
      "   [0.4       ]\n",
      "   [0.80784315]\n",
      "   ...\n",
      "   [0.7137255 ]\n",
      "   [0.00784314]\n",
      "   [0.02745098]]]\n",
      "\n",
      "\n",
      " [[[0.11764706]\n",
      "   [0.        ]\n",
      "   [0.00392157]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.8901961 ]\n",
      "   [0.7254902 ]\n",
      "   [0.08627451]\n",
      "   ...\n",
      "   [0.00392157]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.03921569]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.00392157]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9882353 ]\n",
      "   [0.65882355]\n",
      "   [0.9490196 ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.95686275]\n",
      "   [0.9490196 ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.49411765]\n",
      "   [0.62352943]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.78039217]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.28627452]\n",
      "   [0.        ]\n",
      "   [0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "model_event_10 = cifar10_cnn('cnn_event_model_10epochs.h5', 'data_img')\n",
    "model_event_10.run_test_harness(epochs=10, batch_size=64)\n",
    "\n",
    "trainX, trainY, testX, testY = model_event_10.load_dataset_from_directory()\n",
    "\n",
    "print(trainX)\n",
    "\n",
    "train_norm = trainX.astype('float32')\n",
    "train_norm = train_norm / 255.0\n",
    "print(train_norm)\n",
    "\n",
    "#result = model_event_10.model.predict(testX)\n",
    "#classes_x=np.argmax(result,axis=1)\n",
    "#print(classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fa4611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask_ml\n",
    "print(dask_ml.__version__)\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0330714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
